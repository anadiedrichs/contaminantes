---
title: "Data analysis & modeling report"
format: 
  html: default
  pdf: default
editor: source
---

```{r echo=FALSE}

# Restaura el entorno renv si existe el lockfile
if (file.exists("renv.lock")) {
  if (!requireNamespace("renv", quietly = TRUE)) {
    install.packages("renv")
  }
  renv::restore()
}

```


# Required packages
```{r}
library(readr)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(tidymodels)
library(glmnet)
library(vip)
library(ranger)
library(readxl)
library(kernelshap)
library(shapviz)
```

# Load dataset

```{r}

database <- read_excel("database_2021_2024_con out.xlsx", 
    col_types = c("date", "numeric", "text", 
        "date", "numeric", "numeric", "numeric", 
        "skip", "skip", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric"))

colnames(database)
```

```{r}
nrow(database)
```

```{r}
summary(database)
```

# Cleaning

## Delete columns
- Keep variables Temperatura, Humedad relativa, Presión atmosférica, Velocidad de viento, CO, NO, NO2, O3.
- Delete column NOX 
- Delete NA

```{r}
data <- database %>% select(-one_of(c("Estación","Dia","Año","Hora","NOX (ug/m3)"))) 

```
##  Clean the features' names

```{r}
library(stringr)
colnames(data) <- str_replace(colnames(data),pattern="\\s+\\(\\S+", "")
```

## Missing values

```{r}
# visualizar los valores perdidos del dataset data. Instalar la libreria naniar si no está instalada
if(!require(naniar)){
  install.packages("naniar")
  library(naniar)
}

vis_miss(data)
```

```{r}
gg_miss_var(data)
```

```{r}
data_imputed <- data %>%
  mutate(
    PM10 = ifelse(is.na(PM10), median(PM10, na.rm = TRUE), PM10),
    Presión = ifelse(is.na(Presión), median(Presión, na.rm = TRUE), Presión)
  )
```

* The dataset has `{r} nrow(data_imputed)` samples (rows) and `{r} ncol(data_imputed)` variables (columns).

* The features (variables) are: `{r} colnames(data_imputed)`.

## Variable name

```{r}
data <- data_imputed
colnames(data)
```


* Translate names to English

```{r}
colnames(data)[1:4] <- c("Temperature","Humidity","Pressure","Wind speed")
```

```{r}
colnames(data)
```


* Data summary

```{r}
summary(data)
```

* The variable Particulate Matter (PM10) will be discretized using a threshold value of 45 µg/m3, below which it will be categorized as "Good." Above 45 µg/m3, it will be assigned the value "Bad."


```{r}
y_col_name <- colnames(data)[10]
y_cut <- cut(data$PM10,breaks=c(-10,45,400),labels = c("Good","Bad"))
data$PM10 <- y_cut
```

# EDA 

## Pearson correlation

```{r}
plot_correlation(data)
```

```{r}
ggsave(filename = "./figs-to-paper/01-pearson-correlation.tiff",units = "px", dpi=300)
```

## Box plot
```{r}

plot_boxplot(data, by = "PM10")

```

```{r}
ggsave(filename = "./figs-to-paper/02-boxplot.tiff",units = "px", dpi=300)
```

```{r}

plot_bar(data)
```


```{r}
ggsave(filename = "./figs-to-paper/03-dataset-desbalanceado.tiff",units = "px", dpi=300)
```


# Machine learning models

## Slipt train & test set

We split the dataset to use a 75 % for training and a 25 % for testing.


```{r}

set.seed(123)
splits      <- initial_split(data, strata = PM10, prop = 3/4) 

data_train <- training(splits) # 75 % train set
data_test  <- testing(splits)  # 25 % testing set
```

# Binary clasification with Logistic regression

* We train two models to develop the binary classifier: logistic regression and random forest. 
* We will use the tidymodels library.
* The logistic regression model is implemented in the glmnet package.
* mixture = 1 means L1 regularization (a pure Lasso model) will be used. A mixture value of 1 means that the glmnet model will potentially eliminate irrelevant predictors and choose a simpler model.
* penalty: This hyperparameter represents how much of this regularization we will use. We will adjust it during training to find the best value for making predictions with our data.

```{r}
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") # we use the package glmnet
```

## Tidymodels recipe

* data_train dataset will be used to train the logistic model. We want to predict the variable PM10.

* step_normalize() creates a specification of a recipe step that will normalize numeric data to have a standard deviation of one and a mean of zero.

```{r}
lr_recipe <- 
  recipe(PM10 ~ ., data = data_train) %>% 
  step_normalize(all_predictors())
```

## Tidymodels workflow


```{r}
lr_workflow <- 
  workflow() %>%  # create a workflow
  add_model(lr_mod) %>% # add the model
  add_recipe(lr_recipe) # add the recipe
```

## Grid tunning


We have a hyperparameter to adjust: the penalty for L1 regularization. We can configure the grid manually using a one-column table with 30 candidate values.

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

```

List of the different values for the penalty hyperparameter to try in the training phase.

```{r}
lr_reg_grid
```
## Validation set 

With the strata argument, random sampling is performed within the variable PM10 (the stratification variable). This can help ensure that the new samples have proportions equivalent to those in the original dataset. In the case of a categorical variable like PM10, sampling is performed separately within each class.

A validation dataset is used to tune the penalty hyperparameter.
Within the training dataset, 80% is kept for training and 20% for validation.

Within the training dataset, we use a portion of it as a validation set to train with the different penalty values ​​(the grid tuning we will perform).


```{r}
set.seed(234)
# 20 %
val_set <- validation_split(data_train, 
                            strata = PM10, 
                            prop = 0.80)
```

## Training execution 

* The following code block executes everything: the recipe or instructions saved in `lr_workflow` plus the hyperparameter tuning (tune_grid).

* ROC_AUC is the classifier evaluation metric.

```{r}
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))


```

```{r}
lr_res
```

## Hyperparameter tuning results

We plot the variation in ROC values for different penalty values.

The higher the ROC value, the better the models.

```{r}
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 
```

```{r}
ggsave(filename = "./figs-to-paper/04-log-reg-tunning-results.tiff",units = "px", dpi=300)
```



The graph above shows that model performance is generally better with lower penalty values, which suggests that most predictors are important to the model. 

We also see a steep drop in the area under the ROC curve toward higher penalty values, which happens because a large enough penalty will remove all predictors from the model, and, as expected, predictive accuracy plummets with fewer predictors in the model.

## Best Logistic Regression Models

We display the top 15 models based on the ROC metric using show_best.
The higher the ROC value, the better the models.

The data is displayed in order from lowest to highest penalty value.

```{r}
top_models <-
  lr_res %>% 
  show_best(metric = "roc_auc", n = 15) %>% 
  arrange(penalty) 
top_models

```

The same information as above, but this time sorted by descending ROC_AUC value.

The data is displayed sorted from lowest to highest according to the penalty value.


```{r}
top_models %>% 
  arrange(desc(mean)) 

```



```{r}
penalty_value <-  lr_res %>% 
  select_best(metric = "roc_auc") %>% # find the best hyperparameter combination given a performance measure
  select(penalty)
```

The best logistic regression model is the one with penalty = `{r} penalty_value`.

We observed minimal ROC variation at the other penalty values.

```{r}
lr_best <- lr_res %>% 
        collect_metrics() %>% 
        arrange(desc(mean)) %>%
        slice(1)
lr_best
```

## Roc curve in training set


```{r}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(PM10, .pred_Good) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```
```{r}
ggsave(filename = "./figs-to-paper/05-log-reg-ROC-best-on-training.tiff",units = "px", dpi=300)
```

## Results (test set)

Get the best model.

```{r}
best_model <- lr_res %>% 
  select_best(metric = "roc_auc") 

best_model
```
Update the workflow.

```{r}
final_wf <- lr_workflow %>%
            finalize_workflow(best_model)

final_wf
```

We can use the last_fit() function with our finalized model; this function fits the finalized model on the full training dataset and evaluates the finalized model on the test data.

```{r}
final_fit <- 
  final_wf %>%
  last_fit(splits) 
```

```{r}
final_fit %>%
  collect_metrics()
```
### ROC CURVE IN TEST SET

```{r}
final_fit %>%
  collect_predictions() %>% 
  roc_curve(PM10, .pred_Good) %>% 
  mutate(model = "Logistic Regression") %>% 
  autoplot()
```
```{r}
ggsave(filename = "./figs-to-paper/06-log-reg-ROC-best-on-testing.tiff",units = "px", dpi=300)
```

We use the best LR model to predict on the test set.

```{r}

lr_model <- extract_workflow(final_fit)

# Class prediction
pred_class <- predict(lr_model,
                      new_data = data_test,
                      type = "class")

# Prediction Probabilities
pred_proba <- predict(lr_model,
                      new_data = data_test,
                      type = "prob")
```

```{r}
lr_results <- data_test %>%
              select(PM10) %>%
              bind_cols(pred_class, pred_proba)

```

### Confusion matrix ( on test set)

```{r}
conf_mat(lr_results, truth = PM10,
         estimate = .pred_class)
```


#### Accuracy, sensitivity, specificity, etc

```{r}
summary(conf_mat(lr_results, truth = PM10,
         estimate = .pred_class))
```
#### F-measure

```{r}
f_meas(lr_results, truth = PM10,
         estimate = .pred_class)
```


# Random Forest

We train a random forest model for binary classification.

We use the random forest implementation from the ranger package.

We tune two hyperparameters: mtry and min_n, in training time.

We set the parameter trees to 100.


```{r}

# detect the number of cores from the CPU
cores <- parallel::detectCores()

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% 
  set_engine("ranger", num.threads = cores) %>% # use the ranger package
  set_mode("classification") # classification task
```

## Tidymodels recipe


* data_train dataset will be used to train the RF model. 
* We want to predict the variable PM10.

```{r}
rf_recipe <- 
  recipe(PM10 ~ ., data = data_train)
```

## Tidymodels workflow setup

```{r}
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% ## add model RF
  add_recipe(rf_recipe) ## add recipe
```

## RF training 

Within the training dataset, we use a portion of it as a validation set to tune the hyperparameters (mtry and min_n).

```{r}
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25, # grid size
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

## RF hyperparameter tunning results

Results for each value of mtry and min_n:

```{r}
rf_res %>% collect_metrics()
```

Plot hyperparameter tunning results.

```{r}
autoplot(rf_res)
```
```{r}
ggsave(filename = "./figs-to-paper/07-RF-tunning.tiff",units = "px", dpi=300)
```

## RF best models

List the best RF models.

```{r}
rf_res %>% 
  show_best(metric = "roc_auc")
```

We show the best one.

```{r}
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best
```

## RF ROC curve dataset generation



```{r}
# Para filtrar las predicciones solo para nuestro mejor modelo, podemos usar el argumento de parámetros y pasarle nuestro tibble con los mejores valores de hiperparámetros del ajuste, al que llamamos rf_best:

rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(PM10, .pred_Good) %>% 
  mutate(model = "Random Forest")
```

## ROC curve to compare Logistic regression model vs Random Forest model .

```{r}
bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```

```{r}
ggsave(filename = "./figs-to-paper/08-ROC-on-training-set-by-model.tiff",units = "px", dpi=300)
```

Conclusion: the RF model was superior across the entire event probability threshold.

## Final results



We built a model with the selected parameters, trained it, then predict using the test set.

```{r}
# the last model
last_rf_mod <- 
  rand_forest(mtry = rf_best$mtry, min_n = rf_best$min_n, trees = 100) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit
```

### ROC curve plot for Random Forest ( test set)

```{r}
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(PM10, .pred_Good) %>% 
  autoplot()
```


```{r}
ggsave(filename = "./figs-to-paper/09-ROC-RF-testset.tiff",units = "px", dpi=300)
```

## Random Forest confusion matrix (test set) 

```{r}

RF_model <- extract_workflow(last_rf_fit)

# Class prediction
pred_class_rf <- predict(RF_model,
                      new_data = data_test,
                      type = "class")

# Prediction Probabilities
pred_proba_rf <- predict(RF_model,
                      new_data = data_test,
                      type = "prob")
```

```{r}
RF_results <- data_test %>%
              select(PM10) %>%
              bind_cols(pred_class_rf, pred_proba_rf)

```

Print the confusion matrix.


```{r}
conf_mat(RF_results, truth = PM10,
         estimate = .pred_class)
```

## Summary of metrics

```{r}
summary(conf_mat(RF_results, truth = PM10,
         estimate = .pred_class))
```



## Variable importance Score

Compute the VIP.

When using the vip function with a random forest model, the default method computes the mean decrease in impurity (or Gini importance) for each variable. This is calculated by accumulating the improvement in the split criterion at each split in each tree, and normalizing by the standard deviation of the differences.

```{r}

last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 10) # check vip package documentation
```
```{r}
ggsave(filename = "./figs-to-paper/10-RF-VIP.tiff",units = "px", dpi=300)
```

## SHAP values 

Recordamos que rf_best no es un objeto del modelo entrenado, sino un conjunto de hiperparámetros seleccionados que representan el mejor modelo.
Requiero un objeto de la libreria ranger que represente el mejor modelo randoom forest, para luego ser usado para SHAP 

### Setup and computing

```{r}
rf_best <- rf_res %>% select_best(metric = "roc_auc")
# hiperparámetros "ganadores"
rf_best
```


```{r}
# workflow setup: update the hyperparameters
final_rf_workflow <- rf_workflow %>% finalize_workflow(rf_best)

# Train a RF model
rf_final_fit <- final_rf_workflow %>% fit(data = data_train)


```

```{r}
# Get the final model (a ranger::ranger object)
modelo_final <- extract_fit_parsnip(rf_final_fit)$fit
modelo_final
```

`modelo_final` is an object of the class `{r} class(modelo_final)`

```{r}
class(modelo_final)
```

We create a dataset called X with all the predictors features (without PM10)

```{r}
library(fastshap)
library(shapviz)

# Creo X (sin PM10, solo predictoras)
X <- data %>% select(-PM10)

head(X)

```

#### Prediction function definition for class Good


```{r}
# 

pred_fun_good <- function(object, newdata) {
  # get the probabilities matrix
  prob_matrix <- predict(object, data = newdata, response = "prob")
  return(prob_matrix$predictions[,1]) # return predictions for class Good
  # return(prob_matrix$predictions[,2]) # return predictions for class Bad
  
}


```

```{r}
# pred_fun(modelo_final, X)$predictions

```

#### Prediction function definition for class Bad

```{r}
pred_fun_bad <- function(object, newdata) {
  # get the probabilities matrix
  prob_matrix <- predict(object, data = newdata, response = "prob")
  return(prob_matrix$predictions[,2]) # return predictions for class Bad
  
}
```


#### SHAP values for class Good

Calculate SHAP values with fastshap package

```{r }

# Cálculo de SHAP values con fastshap
shap_values_good <- fastshap::explain(
  object = modelo_final,
  X = X,
  pred_wrapper = pred_fun_good,
  nsim = 100, # Aumentar para mayor precisión
)

```

#### SHAP values for class Bad


```{r }

# Cálculo de SHAP values con fastshap
shap_values_bad <- fastshap::explain(
  object = modelo_final,
  X = X,
  pred_wrapper = pred_fun_bad,
  nsim = 100, # Aumentar para mayor precisión
)

```

### Plots for class Good

Importance plot.

```{r}
shap <- shapviz(shap_values_good, X = X )

# Gráfico de importancia
sv_importance(shap)

```
```{r}
 sv_importance(shap,show_numbers = TRUE) + 
  ggtitle("SHAP importance")
```
```{r}
ggsave(filename = "./figs-to-paper/12-SHAP_importance_good.tiff",units = "px", dpi=300)
```

```{r}
sv_importance(shap,"bee")
```
```{r}
ggsave(filename = "./figs-to-paper/13-SHAP_importance_bee_good.tiff",units = "px", dpi=300)
```


Plot interaction

```{r}
colnames(data)[-9]
```

```{r}
sv_dependence(shap, colnames(data)[-9])
```
```{r}
ggsave(filename = "./figs-to-paper/14-SHAP_dependence_good.tiff",units = "px", dpi=300)
```


### Plots for class Bad

Importance plot.

```{r}
shap <- shapviz(shap_values_bad, X = X )

# Gráfico de importancia
sv_importance(shap)

```
```{r}
 sv_importance(shap,show_numbers = TRUE) + 
  ggtitle("SHAP importance")
```
```{r}
ggsave(filename = "./figs-to-paper/12-SHAP_importance_bad.tiff",units = "px", dpi=300)
```

```{r}
sv_importance(shap,"bee")
```
```{r}
ggsave(filename = "./figs-to-paper/13-SHAP_importance_bee_bad.tiff",units = "px", dpi=300)
```


Plot interaction

```{r}
colnames(data)[-9]
```

```{r}
sv_dependence(shap, colnames(data)[-9])
```
```{r}
ggsave(filename = "./figs-to-paper/14-SHAP_dependence_bad.tiff",units = "px", dpi=300)
```

# FINAL RESULTS 

## ROC CURVE


```{r}
lr_auc_f <- final_fit %>%
  collect_predictions() %>% 
  roc_curve(PM10, .pred_Good) %>% 
  mutate(model = "Logistic Regression")

```

```{r}
rf_auc_f <- last_rf_fit %>% 
            collect_predictions() %>% 
            roc_curve(PM10, .pred_Good) %>%  
            mutate(model = "Random Forest")
```


```{r}
bind_rows(rf_auc_f, lr_auc_f) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```
```{r}
ggsave(filename = "./figs-to-paper/11-ROC-on-test-set.tiff",units = "px", dpi=300)
```

# Metrics

Agrego una fila con el resultado ROC_AUC 

```{r}
roc_auc(lr_results, truth = PM10, .pred_Good)
```

```{r}
m_lr <- summary(conf_mat(lr_results, truth = PM10,
         estimate = .pred_class)) %>% 
        bind_rows(roc_auc(lr_results, truth = PM10, .pred_Good)) %>%
        mutate(model = "Logistic Regression")
```


```{r}
m_lr
```


```{r}
roc_auc(RF_results, truth = PM10, .pred_Good)
```
```{r}
m_rf <- summary(conf_mat(RF_results, truth = PM10,estimate = .pred_class)) %>%  
        bind_rows(roc_auc(RF_results, truth = PM10, .pred_Good))  %>%
        mutate(model = "Random Forest")
```

```{r}
m_rf
```


## Results table to compare each model


```{r}
bind_rows(m_rf, m_lr) %>% 
  select(-one_of(c(".estimator")) ) %>% 
  pivot_wider( names_from = "model", values_from = ".estimate")
```


Save the results in a csv file.

```{r}
bind_rows(m_rf, m_lr) %>% 
  select(-one_of(c(".estimator")) ) %>% 
  pivot_wider( names_from = "model", values_from = ".estimate") %>%
  write_csv(file="./figs-to-paper/final-results.csv")
```

# R session info

```{r}
sessionInfo()
```

